{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install, if necessary, and import required libraries --- #\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier # played a little bit, but RF performed better\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Declare working directory --- #\n",
    "path = os.getcwd() # not used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load dataset --- #\n",
    "df = pd.read_csv(\"trainingSet_randomised.csv\")\n",
    "del df[\"Unnamed: 7\"] #drop NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory\n",
    "# Remember to say about class balance 50/50 filipidis and why RF was picked\n",
    "# Explain why non normalisation (not useful in RF)\n",
    "# Explain why I chose RF, good and bad (not just better performance...)\n",
    "# Overfit and say 92.5% was optimal (max benchmark) when using y_train instead of y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate additional variables / features --- #\n",
    "df[\"point_usage\"] = df[\"sum_collect_points\"]/df[\"sum_redeem_points\"]\n",
    "df[\"point_usage_frequency\"] = df[\"sum_collect\"]/df[\"sum_redeem\"]\n",
    "df[\"average_collected_points_per_year\"] = df[\"sum_collect_points\"]/df[\"years_in_the_program\"]\n",
    "df[\"average_redeemed_points_per_year\"] = df[\"sum_redeem_points\"]/df[\"years_in_the_program\"]\n",
    "df[\"average_collections_per_year\"] = df[\"sum_collect\"]/df[\"years_in_the_program\"]\n",
    "df[\"average_redemptions_per_year\"] = df[\"sum_redeem\"]/df[\"years_in_the_program\"]\n",
    "features = [x for x in df.columns if x != \"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Hyper Parameters Setting --- #\n",
    "lr = LinearRegression(fit_intercept=True, normalize=False) # explain why this was bad (could have goine polynomial...)\n",
    "rf = RandomForestClassifier()\n",
    "    # parameters established after Grid Search Cross Validation - seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Encode categorical variables - If selected model requires it --- #\n",
    "#none one-hot encoding applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model's k-fold cross validation evaluation procedure--- #\n",
    "\n",
    "# param_grid = { \n",
    "#     'n_estimators': [200, 300, 400],\n",
    "#     'min_samples_split': [5, 10],\n",
    "#     'max_depth': [None, 5, 10]\n",
    "# }\n",
    "\n",
    "# clf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=9) # accuracy used as metric (9 chosen because of splits 500 each)\n",
    "# print(clf.best_params_)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200, criterion=\"gini\", max_depth=None, max_features=\"sqrt\", min_samples_split=10, random_state=70)\n",
    "    # parameters established after Grid Search Cross Validation - seen above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split in training/validating and testing --- #\n",
    "y = df[\"state\"].values\n",
    "del df[\"state\"]\n",
    "X = df.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=10,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=70, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Model building --- #\n",
    "rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate prediction --- #\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is 86.4%.\n"
     ]
    }
   ],
   "source": [
    "# --- Print metrics and confusion matrix --- #\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy score is {}%.\".format(100*accuracy))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "#plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8640764270054888"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Compute and plot Area Under Curve (AUC) --- #\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=None)\n",
    "auc(fpr, tpr)\n",
    "# plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08650068 0.0023354  0.0737892  0.00398402 0.02610294 0.48227153\n",
      " 0.06528522 0.08081451 0.06649115 0.02380293 0.06338339 0.02523904]\n",
      "['sum_collect', 'sum_redeem', 'sum_collect_points', 'sum_redeem_points', 'years_in_the_program', 'months_since_last_transaction', 'point_usage', 'point_usage_frequency', 'average_collected_points_per_year', 'average_redeemed_points_per_year', 'average_collections_per_year', 'average_redemptions_per_year']\n"
     ]
    }
   ],
   "source": [
    "# --- Variable Importance Plot --- #\n",
    "print(rf.feature_importances_)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save final Model to a file --- #\n",
    "filename = 'propensity_model.pkl'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
